{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"chenweiting_512_66_DA\"\n",
    "network_weight = \"\" # leave empty to use all the loras in the dir \n",
    "inference_ckpt = \"/home/ubuntu/stable-diffusion-webui/models/Stable-diffusion/chilloutmix_NiPrunedFp32Fix.safetensors\"\n",
    "vae = \"/home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\" \n",
    "network_mul = 1  # @param {type:\"slider\", min:-1, max:2, step:0.05}\n",
    "\n",
    "instance_prompt = \"\"  # @param {type: \"string\"}\n",
    "prompt = \"masterpiece, 1 man in white, upper body\"  # @param {type: \"string\"}\n",
    "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"  # @param {type: \"string\"}\n",
    "\n",
    "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
    "sampler = \"ddim\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "steps = 28  # @param {type: \"slider\", min: 1, max: 100}\n",
    "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
    "width = 512  # @param {type: \"integer\"}\n",
    "height = 512  # @param {type: \"integer\"}\n",
    "images_per_prompt = 4  # @param {type: \"integer\"}\n",
    "batch_size = 2  # @param {type: \"integer\"}\n",
    "clip_skip = 1  # @param {type: \"slider\", min: 1, max: 40}\n",
    "seed = -1  # @param {type: \"integer\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/ubuntu/alex/lora\"\n",
    "v2 = False  # @param {type:\"boolean\"}\n",
    "v_parameterization = False  # @param {type:\"boolean\"}\n",
    "\n",
    "network_module = \"networks.lora\"\n",
    "network_args = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting-000002.safetensors\n",
      "load StableDiffusion checkpoint\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/_utils.py:777: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/storage.py:955: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "loading text encoder: <All keys matched successfully>\n",
      "load VAE: /home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\n",
      "additional VAE loaded\n",
      "Replace CrossAttention.forward to use NAI style Hypernetwork and FlashAttention\n",
      "loading tokenizer\n",
      "prepare tokenizer\n",
      "import network module: networks.lora\n",
      "load network weights from: /home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting-000002.safetensors\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "create LoRA for U-Net: 192 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "weights are loaded: <All keys matched successfully>\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:450: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:463: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "pipeline is ready.\n",
      "iteration 1/1\n",
      "prompt 1/1: masterpiece, 1 man in white, upper body\n",
      "negative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.60it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.81it/s]\n",
      "done!\n",
      "/home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting-000001.safetensors\n",
      "load StableDiffusion checkpoint\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/_utils.py:777: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/storage.py:955: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "loading text encoder: <All keys matched successfully>\n",
      "load VAE: /home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\n",
      "additional VAE loaded\n",
      "Replace CrossAttention.forward to use NAI style Hypernetwork and FlashAttention\n",
      "loading tokenizer\n",
      "prepare tokenizer\n",
      "import network module: networks.lora\n",
      "load network weights from: /home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting-000001.safetensors\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "create LoRA for U-Net: 192 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "weights are loaded: <All keys matched successfully>\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:450: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:463: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "pipeline is ready.\n",
      "iteration 1/1\n",
      "prompt 1/1: masterpiece, 1 man in white, upper body\n",
      "negative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.60it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.81it/s]\n",
      "done!\n",
      "/home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting.safetensors\n",
      "load StableDiffusion checkpoint\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/_utils.py:777: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/storage.py:955: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "loading text encoder: <All keys matched successfully>\n",
      "load VAE: /home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\n",
      "additional VAE loaded\n",
      "Replace CrossAttention.forward to use NAI style Hypernetwork and FlashAttention\n",
      "loading tokenizer\n",
      "prepare tokenizer\n",
      "import network module: networks.lora\n",
      "load network weights from: /home/ubuntu/alex/lora/chenweiting_512_66_DA/output/chenweiting.safetensors\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "create LoRA for U-Net: 192 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "weights are loaded: <All keys matched successfully>\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:450: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:463: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.10.2\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "pipeline is ready.\n",
      "iteration 1/1\n",
      "prompt 1/1: masterpiece, 1 man in white, upper body\n",
      "negative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.57it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.80it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
    "output_dir = os.path.join(root_dir, dir_name, \"output\")\n",
    "inference_dir = os.path.join(output_dir, \"inference\")\n",
    "\n",
    "final_prompt = (\n",
    "    f\"{instance_prompt}, {prompt} --n {negative}\"\n",
    "    if instance_prompt\n",
    "    else f\"{prompt} --n {negative}\"\n",
    ")\n",
    "\n",
    "if network_weight:\n",
    "    lora_files = [network_weight]\n",
    "\n",
    "else:\n",
    "    os.chdir(output_dir)\n",
    "    lora_files = [os.path.abspath(file) for file in glob.glob('*.safetensors')]\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "for network_weight in lora_files:\n",
    "    print(network_weight)\n",
    "    config = {\n",
    "        \"v2\": v2,\n",
    "        \"v_parameterization\": v_parameterization,\n",
    "        \"network_module\": network_module,\n",
    "        \"network_weight\": network_weight,\n",
    "        \"network_mul\": float(network_mul),\n",
    "        \"network_args\": eval(network_args) if network_args else None,\n",
    "        \"ckpt\": inference_ckpt,\n",
    "        \"outdir\": inference_dir,\n",
    "        \"xformers\": False,\n",
    "        \"vae\": vae if vae else None,\n",
    "        \"fp16\": True,\n",
    "        \"W\": width,\n",
    "        \"H\": height,\n",
    "        \"seed\": seed if seed > 0 else None,\n",
    "        \"scale\": scale,\n",
    "        \"sampler\": sampler,\n",
    "        \"steps\": steps,\n",
    "        \"max_embeddings_multiples\": 3,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"images_per_prompt\": images_per_prompt,\n",
    "        \"clip_skip\": clip_skip if not v2 else None,\n",
    "        \"prompt\": final_prompt,\n",
    "    }\n",
    "\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        if isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        if isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        if isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    final_args = f\"python gen_img_diffusers.py {args}\"\n",
    "    !{final_args}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365aaa7cd0f46b98e87a641efd2674b70fd0636216a71fca34e134fee9533f7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
