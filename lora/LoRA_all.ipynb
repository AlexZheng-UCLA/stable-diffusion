{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "# **Kohya LoRA Dreambooth**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# directory \n",
        "dir_name = \"chenweiting_512_66_DA_combined\"\n",
        "project_name = \"chenweiting\"\n",
        "data_name = \"chenweiting/chenweiting-512\"\n",
        "\n",
        "# model\n",
        "training_ckpt = \"/home/ubuntu/stable-diffusion-webui/models/Stable-diffusion/hassanblend14.safetensors\"\n",
        "inference_ckpt = \"/home/ubuntu/stable-diffusion-webui/models/Stable-diffusion/hassanblend14.safetensors\"\n",
        "vae = \"/home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\"  \n",
        "\n",
        "# dataset\n",
        "instance_token = \"chenweiting\" \n",
        "class_token = \"man\"  \n",
        "add_token_to_caption = False\n",
        "resolution = 512  \n",
        "flip_aug = False \n",
        "data_anotation = \"combined\"  # @param [\"none\", \"waifu\", \"blip\", \"combined\"]\n",
        "caption_extension = \".combined\"  # @param [\"none\", \".txt\", \".caption\", \"combined\"]\n",
        "\n",
        "# training \n",
        "train_repeats = 66  \n",
        "reg_repeats = 0\n",
        "num_epochs = 3  # @param {type:\"number\"}\n",
        "train_batch_size = 2  # @param {type:\"number\"}\n",
        "network_dim = 64  \n",
        "network_alpha = 64\n",
        "save_n_epochs_type = \"save_every_n_epochs\"  # @param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"]\n",
        "save_n_epochs_type_value = 1  # @param {type:\"number\"}\n",
        "lr_scheduler = \"polynomial\"  #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"]\n",
        "lowram = False \n",
        "\n",
        "# sampling\n",
        "sampler = \"k_dpm_2\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "sample_str = f\"\"\"\n",
        "  masterpiece, best quality, 1boy, black eyes, white shirt, hat, looking at viewer, shirt, solo, upper body\\\n",
        "  --n lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry \\\n",
        "  --w 512 \\\n",
        "  --h 512 \\\n",
        "  --l 7 \\\n",
        "  --s 28    \n",
        "\"\"\"\n",
        "images_per_prompt = 4\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_dir = \"/home/ubuntu/alex/lora\"\n",
        "v2 = False \n",
        "v_parameterization = False\n",
        "\n",
        "# dataset\n",
        "caption_dropout_rate = 0  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "caption_dropout_every_n_epochs = 0  \n",
        "keep_tokens = 0  \n",
        "\n",
        "# training\n",
        "optimizer_type = \"DAdaptation\"  # @param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "train_unet = True  \n",
        "unet_lr = 1  \n",
        "train_text_encoder = True\n",
        "text_encoder_lr = 0.5\n",
        "prior_loss_weight = 1.0\n",
        "# @markdown Additional arguments for optimizer, e.g: `[\"decouple=true\",\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"\"  # @param {'type':'string'}\n",
        "\n",
        "lr_scheduler_num_cycles = 0  # @param {'type':'number'}\n",
        "lr_scheduler_power = 1 \n",
        "lr_warmup_steps = 0 \n",
        "noise_offset = 0.0  # @param {type:\"number\"}\n",
        "\n",
        "# sample \n",
        "enable_sample_prompt = True  \n",
        "\n",
        "# \n",
        "mixed_precision = \"fp16\"  # @param [\"no\",\"fp16\",\"bf16\"]\n",
        "save_precision = \"fp16\"  # @param [\"float\", \"fp16\", \"bf16\"] \n",
        "save_model_as = \"safetensors\"  # @param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "max_token_length = 225  # @param {type:\"number\"}\n",
        "clip_skip = 1  # @param {type:\"number\"}\n",
        "gradient_checkpointing = False  # @param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1  # @param {type:\"number\"}\n",
        "seed = -1  # @param {type:\"number\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "\n",
        "\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir, dir_name)\n",
        "dataset_dir = os.path.join(root_dir, \"dataset\", data_name)\n",
        "train_data_dir = os.path.join(training_dir, \"train_data\")\n",
        "reg_data_dir = os.path.join(training_dir, \"reg_data\")\n",
        "config_dir = os.path.join(training_dir, \"config\")\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir = os.path.join(repo_dir, \"finetune\")\n",
        "output_dir = os.path.join(training_dir, \"output\")\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "inference_dir = os.path.join(output_dir, \"inference\")\n",
        "logging_dir = os.path.join(training_dir, \"log\")\n",
        "\n",
        "shutil.copytree(dataset_dir, train_data_dir, dirs_exist_ok=True)\n",
        "\n",
        "\n",
        "for dir in [\n",
        "    training_dir, \n",
        "    config_dir,\n",
        "    output_dir, \n",
        "    sample_dir,\n",
        "    inference_dir\n",
        "    ]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "for store in [\n",
        "    \"root_dir\",\n",
        "    \"repo_dir\",\n",
        "    \"training_dir\",\n",
        "    \"train_data_dir\",\n",
        "    \"reg_data_dir\",\n",
        "    \"accelerate_config\",\n",
        "    \"tools_dir\",\n",
        "    \"finetune_dir\",\n",
        "    \"config_dir\",\n",
        "    \"sample_dir\",\n",
        "    \"logging_dir\",\n",
        "    \"inference_dir\",\n",
        "    \"training_ckpt\",\n",
        "    \"inference_ckpt\",\n",
        "    \"vae\"\n",
        "]:\n",
        "    with capture.capture_output() as cap:\n",
        "        %store {store}\n",
        "        del cap\n",
        "\n",
        "from accelerate.utils import write_basic_config\n",
        "\n",
        "if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T-0qKyEgTchp"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "Jz2emq6vWnPu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting file cwt-017.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-026.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-033.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-016.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-024.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-013.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-004.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-035.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-021.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-015.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-008.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-006.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-020.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-032.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-003.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-012.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-005.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-034.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-022.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-014.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-023.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-018.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-030.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-029.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-009.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-028.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-019.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-010.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-011.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-007.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-001.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-002.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-031.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-025.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "Deleting file cwt-027.combined from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1866.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Converted image: cwt-033.png\n",
            " Converted image: cwt-028.png\n",
            "All images have been converted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "%store -r\n",
        "\n",
        "# os.chdir(root_dir)\n",
        "\n",
        "test = os.listdir(train_data_dir)\n",
        "# @markdown This section will delete unnecessary files and unsupported media such as `.mp4`, `.webm`, and `.gif`.\n",
        "\n",
        "supported_types = [\n",
        "    \".png\",\n",
        "    \".jpg\",\n",
        "    \".jpeg\",\n",
        "    \".webp\",\n",
        "    \".bmp\",\n",
        "    \".caption\",\n",
        "    \".combined\"\n",
        "    \".npz\",\n",
        "    \".txt\",\n",
        "    \".json\",\n",
        "]\n",
        "\n",
        "for item in test:\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    if file_ext not in supported_types:\n",
        "        print(f\"Deleting file {item} from {train_data_dir}\")\n",
        "        os.remove(os.path.join(train_data_dir, item))\n",
        "\n",
        "# @markdown ### <br> Convert Transparent Images\n",
        "# @markdown This code will convert your transparent dataset with alpha channel (RGBA) to RGB and give it a white background.\n",
        "\n",
        "convert = True  # @param {type:\"boolean\"}\n",
        "random_color = False  # @param {type:\"boolean\"}\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "images = [\n",
        "    image\n",
        "    for image in os.listdir(train_data_dir)\n",
        "    if image.endswith(\".png\") or image.endswith(\".webp\")\n",
        "]\n",
        "background_colors = [\n",
        "    (255, 255, 255),\n",
        "    (0, 0, 0),\n",
        "    (255, 0, 0),\n",
        "    (0, 255, 0),\n",
        "    (0, 0, 255),\n",
        "    (255, 255, 0),\n",
        "    (255, 0, 255),\n",
        "    (0, 255, 255),\n",
        "]\n",
        "\n",
        "\n",
        "def process_image(image_name):\n",
        "    img = Image.open(f\"{train_data_dir}/{image_name}\")\n",
        "\n",
        "    if img.mode in (\"RGBA\", \"LA\"):\n",
        "        if random_color:\n",
        "            background_color = random.choice(background_colors)\n",
        "        else:\n",
        "            background_color = (255, 255, 255)\n",
        "        bg = Image.new(\"RGB\", img.size, background_color)\n",
        "        bg.paste(img, mask=img.split()[-1])\n",
        "\n",
        "        if image_name.endswith(\".webp\"):\n",
        "            bg = bg.convert(\"RGB\")\n",
        "            bg.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f\"{train_data_dir}/{image_name}\")\n",
        "            print(\n",
        "                f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\"\n",
        "            )\n",
        "        else:\n",
        "            bg.save(f\"{train_data_dir}/{image_name}\", \"PNG\")\n",
        "            print(f\" Converted image: {image_name}\")\n",
        "    else:\n",
        "        if image_name.endswith(\".webp\"):\n",
        "            img.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f\"{train_data_dir}/{image_name}\")\n",
        "            print(\n",
        "                f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\"\n",
        "            )\n",
        "        else:\n",
        "            img.save(f\"{train_data_dir}/{image_name}\", \"PNG\")\n",
        "\n",
        "\n",
        "num_batches = len(images) // batch_size + 1\n",
        "\n",
        "if convert:\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        for i in tqdm(range(num_batches)):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "            batch = images[start:end]\n",
        "            executor.map(process_image, batch)\n",
        "\n",
        "    print(\"All images have been converted\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qdISafLeyklg"
      },
      "source": [
        "## Data Annotation\n",
        "You can choose to train a model using captions. We're using [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) for image captioning and [Waifu Diffusion 1.4 Tagger](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) for image tagging similar to Danbooru.\n",
        "- Use BLIP Captioning for: `General Images`\n",
        "- Use Waifu Diffusion 1.4 Tagger V2 for: `Anime and Manga-style Images`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvGx2Ikhc8iy",
        "outputId": "16e552d4-2633-49a8-b4ca-0470cf0781fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load images from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\n",
            "found 35 images.\n",
            "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "BLIP loaded\n",
            "100%|███████████████████████████████████████████| 18/18 [00:05<00:00,  3.38it/s]\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "%store -r\n",
        "if data_anotation == \"blip\" or data_anotation == \"combined\":\n",
        "\n",
        "    os.chdir(finetune_dir)\n",
        "\n",
        "    batch_size = 2 #@param {type:'number'}\n",
        "    max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "    beam_search = True #@param {type:'boolean'}\n",
        "    min_length = 5 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
        "    max_length = 75 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
        "\n",
        "    !python make_captions.py \\\n",
        "        \"{train_data_dir}\" \\\n",
        "        --batch_size {batch_size} \\\n",
        "        {\"--beam_search\" if beam_search else \"\"} \\\n",
        "        --min_length {min_length} \\\n",
        "        --max_length {max_length} \\\n",
        "        --caption_extension .caption \\\n",
        "        --max_data_loader_n_workers {max_data_loader_n_workers}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BdXV7rAy2ag",
        "outputId": "ac08f30b-6516-4a00-c4a2-04a14cdcda08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using existing wd14 tagger model\n",
            "2023-04-11 06:01:34.611189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:34.644538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:34.645685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:34.646909: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-11 06:01:34.647522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:34.648556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:34.649571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:35.082995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:35.084056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:35.084879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-11 06:01:35.085700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20595 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "found 35 images.\n",
            "  0%|                                                    | 0/18 [00:00<?, ?it/s]2023-04-11 06:01:58.187303: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n",
            "2023-04-11 06:01:58.752369: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "100%|███████████████████████████████████████████| 18/18 [00:06<00:00,  2.62it/s]\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# 4.2.2. Waifu Diffusion 1.4 Tagger V2\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "if data_anotation == \"waifu\" or data_anotation == \"combined\":\n",
        "    os.chdir(finetune_dir)\n",
        "\n",
        "    batch_size = 2 #@param {type:'number'}\n",
        "    max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "    model = \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\" #@param [\"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
        "    #@markdown Use the `recursive` option to process subfolders as well, useful for multi-concept training.\n",
        "    recursive = False #@param {type:\"boolean\"} \n",
        "    #@markdown Debug while tagging, it will print your image file with general tags and character tags.\n",
        "    verbose_logging = False #@param {type:\"boolean\"}\n",
        "    #@markdown Separate `undesired_tags` with comma `(,)` if you want to remove multiple tags, e.g. `1girl,solo,smile`.\n",
        "    undesired_tags = \"\" #@param {type:'string'}\n",
        "    #@markdown  Adjust `general_threshold` for pruning tags (less tags, less flexible). `character_threshold` is useful if you want to train with character tags, e.g. `hakurei reimu`.\n",
        "    general_threshold = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "    character_threshold = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "    config = {\n",
        "        \"_train_data_dir\": train_data_dir,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"repo_id\": model,\n",
        "        \"recursive\": recursive,\n",
        "        \"remove_underscore\": True,\n",
        "        \"general_threshold\": general_threshold,\n",
        "        \"character_threshold\": character_threshold,\n",
        "        \"caption_extension\": \".txt\",\n",
        "        \"max_data_loader_n_workers\": max_data_loader_n_workers,\n",
        "        \"debug\": verbose_logging,\n",
        "        \"undesired_tags\": undesired_tags\n",
        "    }\n",
        "\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    final_args = f\"python tag_images_by_wd14_tagger.py {args}\"\n",
        "\n",
        "    os.chdir(finetune_dir)\n",
        "    !{final_args}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combine BLIP and Waifu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "_mLVURhM9PFE"
      },
      "outputs": [],
      "source": [
        "### Combine Caption \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "# os.chdir(train_data_dir)\n",
        "\n",
        "def read_file_content(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        content = file.read()\n",
        "    return content\n",
        "\n",
        "def remove_redundant_words(content1, content2):\n",
        "    return content1.rstrip('\\n') + ', ' + content2\n",
        "\n",
        "def write_file_content(file_path, content):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.write(content)\n",
        "\n",
        "def main():\n",
        "    directory = train_data_dir\n",
        "    extension1 = \".caption\"\n",
        "    extension2 = \".txt\"\n",
        "    output_extension = \".combined\"\n",
        "\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(extension1):\n",
        "            filename = os.path.splitext(file)[0]\n",
        "            file1 = os.path.join(directory, filename + extension1)\n",
        "            file2 = os.path.join(directory, filename + extension2)\n",
        "            output_file = os.path.join(directory, filename + output_extension)\n",
        "\n",
        "            if os.path.exists(file2):\n",
        "                content1 = read_file_content(file1)\n",
        "                content2 = read_file_content(file2)\n",
        "\n",
        "                combined_content = remove_redundant_words(content1, content2)\n",
        "\n",
        "                write_file_content(output_file, combined_content)\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yHNbl3O_NSS0"
      },
      "source": [
        "## Training Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5u_DhFeyJ6R",
        "outputId": "1b22b893-253b-453a-eadd-114afa5bede2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[datasets]]\n",
            "resolution = 512\n",
            "min_bucket_reso = 256\n",
            "max_bucket_reso = 1024\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "flip_aug = false\n",
            "color_aug = false\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\"\n",
            "class_tokens = \"chenweiting man\"\n",
            "num_repeats = 66\n",
            "\n",
            "[[datasets.subsets]]\n",
            "is_reg = true\n",
            "image_dir = \"/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/reg_data\"\n",
            "class_tokens = \"man\"\n",
            "num_repeats = 0\n",
            "\n",
            "\n",
            "[general]\n",
            "enable_bucket = true\n",
            "caption_extension = \".combined\"\n",
            "shuffle_caption = true\n",
            "keep_tokens = 0\n",
            "bucket_reso_steps = 64\n",
            "bucket_no_upscale = false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5.2. Dataset Config\n",
        "import toml\n",
        "\n",
        "%store -r\n",
        "if add_token_to_caption and keep_tokens < 2:\n",
        "    keep_tokens = 1\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def add_tag(filename, tag):\n",
        "    contents = read_file(filename)\n",
        "    tag = \", \".join(tag.split())\n",
        "    tag = tag.replace(\"_\", \" \")\n",
        "    if tag in contents:\n",
        "        return\n",
        "    contents = tag + \", \" + contents\n",
        "    write_file(filename, contents)\n",
        "\n",
        "def delete_tag(filename, tag):\n",
        "    contents = read_file(filename)\n",
        "    tag = \", \".join(tag.split())\n",
        "    tag = tag.replace(\"_\", \" \")\n",
        "    if tag not in contents:\n",
        "        return\n",
        "    contents = \"\".join([s.strip(\", \") for s in contents.split(tag)])\n",
        "    write_file(filename, contents)\n",
        "\n",
        "if caption_extension != \"none\":\n",
        "    tag = f\"{instance_token}_{class_token}\" if 'class_token' in globals() else instance_token\n",
        "    for filename in os.listdir(train_data_dir):\n",
        "        if filename.endswith(caption_extension):\n",
        "            file_path = os.path.join(train_data_dir, filename)\n",
        "            if add_token_to_caption:\n",
        "                add_tag(file_path, tag)\n",
        "            else:\n",
        "                delete_tag(file_path, tag)\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "            \"caption_dropout_rate\": caption_dropout_rate if caption_extension == \".caption\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".txt\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".combined\" else 0,\n",
        "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"image_dir\": train_data_dir,\n",
        "                    \"class_tokens\": f\"{instance_token} {class_token}\" if 'class_token' in globals() else instance_token,\n",
        "                    \"num_repeats\": train_repeats,\n",
        "                },\n",
        "                {\n",
        "                    \"is_reg\": True,\n",
        "                    \"image_dir\": reg_data_dir,\n",
        "                    \"class_tokens\": class_token if 'class_token' in globals() else None,\n",
        "                    \"num_repeats\": reg_repeats,\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD5Ecamp4rVW",
        "outputId": "73ac0549-440b-4800-b7ba-2dd5a8374adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- LoRA Config:\n",
            "  - Additional network category: LoRA\n",
            "  - Loading network module: networks.lora\n",
            "  - networks.lora linear_dim set to: 64\n",
            "  - networks.lora linear_alpha set to: 64\n",
            "  - No LoRA weight loaded.\n",
            "- Optimizer Config:\n",
            "  - Using DAdaptation as Optimizer\n",
            "  - Train UNet and Text Encoder\n",
            "    - UNet learning rate: 1\n",
            "    - Text encoder learning rate: 0.5\n",
            "  - Learning rate warmup steps: 0\n",
            "  - Learning rate Scheduler: polynomial\n",
            "  - lr_scheduler_power: 1\n"
          ]
        }
      ],
      "source": [
        "# 5.3. LoRA and Optimizer Config\n",
        "\n",
        "%store -r\n",
        "# @markdown ### LoRA Config:\n",
        "network_category = \"LoRA\"  # @param [\"LoRA\", \"LoCon\", \"LoCon_Lycoris\", \"LoHa\"]\n",
        "\n",
        "# @markdown Recommended values:\n",
        "\n",
        "# @markdown | network_category | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "# @markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "# @markdown | LoRA | 32 | 1 | - | - |\n",
        "# @markdown | LoCon | 16 | 8 | 8 | 1 |\n",
        "# @markdown | LoHa | 8 | 4 | 4 | 1 |\n",
        "\n",
        "# @markdown - Currently, `dropout` and `cp_decomposition` is not available in this notebook.\n",
        "\n",
        "# @markdown `conv_dim` and `conv_alpha` are needed to train `LoCon` and `LoHa`, skip it if you train normal `LoRA`. But remember, when in doubt, set `dim = alpha`.\n",
        "conv_dim = 32  # @param {'type':'number'}\n",
        "conv_alpha = 16  # @param {'type':'number'}\n",
        "# @markdown It's recommended to not set `network_dim` and `network_alpha` higher than `64`, especially for LoHa.\n",
        "# @markdown But if you want to train with higher dim/alpha so badly, try using higher learning rate. Because the model learning faster in higher dim.\n",
        "\n",
        "# @markdown You can specify this field for resume training.\n",
        "network_weight = \"\"  # @param {'type':'string'}\n",
        "network_module = \"lycoris.kohya\" if network_category in [\"LoHa\", \"LoCon_Lycoris\"] else \"networks.lora\"\n",
        "network_args = \"\" if network_category == \"LoRA\" else [\n",
        "    f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\",\n",
        "    ]\n",
        "\n",
        "if network_category == \"LoHa\":\n",
        "  network_args.append(\"algo=loha\")\n",
        "elif network_category == \"LoCon_Lycoris\":\n",
        "  network_args.append(\"algo=lora\")\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(f\"  - Additional network category: {network_category}\")\n",
        "print(f\"  - Loading network module: {network_module}\")\n",
        "if not network_category == \"LoRA\":\n",
        "  print(f\"  - network args: {network_args}\")\n",
        "print(f\"  - {network_module} linear_dim set to: {network_dim}\")\n",
        "print(f\"  - {network_module} linear_alpha set to: {network_alpha}\")\n",
        "if not network_category == \"LoRA\":\n",
        "  print(f\"  - {network_module} conv_dim set to: {conv_dim}\")\n",
        "  print(f\"  - {network_module} conv_alpha set to: {conv_alpha}\")\n",
        "\n",
        "if not network_weight:\n",
        "    print(\"  - No LoRA weight loaded.\")\n",
        "else:\n",
        "    if os.path.exists(network_weight):\n",
        "        print(f\"  - Loading LoRA weight: {network_weight}\")\n",
        "    else:\n",
        "        print(f\"  - {network_weight} does not exist.\")\n",
        "        network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"  - Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "    print(f\"  - Optimizer Args: {optimizer_args}\")\n",
        "if train_unet and train_text_encoder:\n",
        "    print(\"  - Train UNet and Text Encoder\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "if train_unet and not train_text_encoder:\n",
        "    print(\"  - Train UNet only\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "if train_text_encoder and not train_unet:\n",
        "    print(\"  - Train Text Encoder only\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "print(f\"  - Learning rate warmup steps: {lr_warmup_steps}\")\n",
        "print(f\"  - Learning rate Scheduler: {lr_scheduler}\")\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "    print(f\"  - lr_scheduler_num_cycles: {lr_scheduler_num_cycles}\")\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "    print(f\"  - lr_scheduler_power: {lr_scheduler_power}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z4w3lfFKLjr",
        "outputId": "f8ff4760-60e0-4074-abe2-af89affb35bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[model_arguments]\n",
            "v2 = false\n",
            "v_parameterization = false\n",
            "pretrained_model_name_or_path = \"/home/ubuntu/stable-diffusion-webui/models/Stable-diffusion/hassanblend14.safetensors\"\n",
            "vae = \"/home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "unet_lr = 1.0\n",
            "text_encoder_lr = 0.5\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 64\n",
            "network_alpha = 64\n",
            "network_train_unet_only = false\n",
            "network_train_text_encoder_only = false\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"DAdaptation\"\n",
            "learning_rate = 1\n",
            "max_grad_norm = 1.0\n",
            "lr_scheduler = \"polynomial\"\n",
            "lr_warmup_steps = 0\n",
            "lr_scheduler_power = 1\n",
            "\n",
            "[dataset_arguments]\n",
            "cache_latents = true\n",
            "debug_dataset = false\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/output\"\n",
            "output_name = \"chenweiting\"\n",
            "save_precision = \"fp16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 2\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "xformers = false\n",
            "max_train_epochs = 3\n",
            "max_data_loader_n_workers = 4\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = false\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"fp16\"\n",
            "clip_skip = 1\n",
            "logging_dir = \"/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/log\"\n",
            "log_prefix = \"chenweiting\"\n",
            "lowram = false\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"k_dpm_2\"\n",
            "images_per_prompt = 4\n",
            "\n",
            "[dreambooth_arguments]\n",
            "prior_loss_weight = 1.0\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5.4. Training Config\n",
        "import toml\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization\n",
        "        if v2 and v_parameterization\n",
        "        else False,\n",
        "        \"pretrained_model_name_or_path\": training_ckpt,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": False,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 4,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\": {\n",
        "        \"sample_every_n_steps\": None,\n",
        "        \"sample_every_n_epochs\": 1 if enable_sample_prompt else 999999,\n",
        "        \"sample_sampler\": sampler,\n",
        "        \"images_per_prompt\": images_per_prompt,\n",
        "    },\n",
        "    \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "write_file(config_path, config_str)\n",
        "write_file(prompt_path, sample_str)\n",
        "    \n",
        "print(config_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_SHtbFwHVl1",
        "outputId": "13eac45e-f1d5-498a-bdbf-d35f45a910cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading settings from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/config/config_file.toml...\n",
            "/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/config/config_file\n",
            "prepare tokenizer\n",
            "update token length: 225\n",
            "Load dataset config from /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/config/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data contains 35 image files\n",
            "ignore subset with image_dir='/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/reg_data': num_repeats is less than 1 / num_repeatsが1を下回っているためサブセットを無視します: 0\n",
            "2310 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 2\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/home/ubuntu/alex/lora/chenweiting_512_66_DA_combined/train_data\"\n",
            "    image_count: 35\n",
            "    num_repeats: 66\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    is_reg: False\n",
            "    class_tokens: chenweiting man\n",
            "    caption_extension: .combined\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100%|█████████████████████████████████████████| 35/35 [00:00<00:00, 5047.82it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (512, 512), count: 2310\n",
            "mean ar error (without repeats): 0.0\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "load StableDiffusion checkpoint\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/_utils.py:777: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/storage.py:955: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\n",
            "additional VAE loaded\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100%|███████████████████████████████████████████| 35/35 [00:02<00:00, 12.94it/s]\n",
            "import network module: networks.lora\n",
            "create LoRA network. base dim (rank): 64, alpha: 64\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "use D-Adaptation Adam optimizer | {}\n",
            "when multiple learning rates are specified with dadaptation (e.g. for Text Encoder and U-Net), only the first one will take effect / D-Adaptationで複数の学習率を指定した場合（Text EncoderとU-Netなど）、最初の学習率のみが有効になります: lr=0.5\n",
            "override steps. steps for 3 epochs is / 指定エポックまでのステップ数: 3465\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 2310\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 1155\n",
            "  num epochs / epoch数: 3\n",
            "  batch size per device / バッチサイズ: 2\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 3465\n",
            "steps:   0%|                                           | 0/3465 [00:00<?, ?it/s]epoch 1/3\n",
            "steps:  19%|███▉                 | 648/3465 [05:23<23:27,  2.00it/s, loss=0.123]"
          ]
        }
      ],
      "source": [
        "%store -r\n",
        "\n",
        "sample_prompt = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "config_file = os.path.join(config_dir, \"config_file.toml\")\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        " \n",
        "os.chdir(repo_dir)\n",
        "!accelerate launch \\\n",
        "  --config_file={accelerate_config} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network.py \\\n",
        "  --sample_prompts={sample_prompt} \\\n",
        "  --dataset_config={dataset_config} \\\n",
        "  --config_file={config_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'STOP_HERE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m STOP_HERE\n",
            "\u001b[0;31mNameError\u001b[0m: name 'STOP_HERE' is not defined"
          ]
        }
      ],
      "source": [
        "STOP_HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMcN0bM_o53"
      },
      "source": [
        "# VI. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKBrTDPrcNjP",
        "outputId": "415d1bfe-68e0-4391-8aef-f4768e43278b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load StableDiffusion checkpoint\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/_utils.py:777: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/home/ubuntu/miniconda3/envs/muse/lib/python3.10/site-packages/torch/storage.py:955: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /home/ubuntu/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.ckpt\n",
            "additional VAE loaded\n",
            "Replace CrossAttention.forward to use NAI style Hypernetwork and FlashAttention\n",
            "loading tokenizer\n",
            "prepare tokenizer\n",
            "import network module: networks.lora\n",
            "load network weights from: /home/ubuntu/alex/lora/training2/output/training2.safetensors\n",
            "create LoRA network from weights\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "weights are loaded: <All keys matched successfully>\n",
            "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:450: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.10.2\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": true,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": true,\n",
            "  \"steps_offset\": 0,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "/home/ubuntu/alex/lora/kohya-trainer/gen_img_diffusers.py:463: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.10.2\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": true,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": true,\n",
            "  \"steps_offset\": 1,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "pipeline is ready.\n",
            "iteration 1/1\n",
            "prompt 1/1: zyc, 1 man in white\n",
            "negative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
            "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.59it/s]\n",
            "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.80it/s]\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 6.3. Inference\n",
        "import os\n",
        "import glob\n",
        "%store -r\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "network_weight = \"/home/ubuntu/alex/lora/training2/output/training2.safetensors\"  # @param {'type':'string'}\n",
        "network_mul = 1  # @param {type:\"slider\", min:-1, max:2, step:0.05}\n",
        "network_module = \"networks.lora\"\n",
        "network_args = \"\"\n",
        "\n",
        "v2 = False  # @param {type:\"boolean\"}\n",
        "v_parameterization = False  # @param {type:\"boolean\"}\n",
        "instance_prompt = \"zyc\"  # @param {type: \"string\"}\n",
        "prompt = \"1 man in white\"  # @param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28  # @param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512  # @param {type: \"integer\"}\n",
        "height = 512  # @param {type: \"integer\"}\n",
        "images_per_prompt = 4  # @param {type: \"integer\"}\n",
        "batch_size = 2  # @param {type: \"integer\"}\n",
        "clip_skip = 1  # @param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1  # @param {type: \"integer\"}\n",
        "\n",
        "final_prompt = (\n",
        "    f\"{instance_prompt}, {prompt} --n {negative}\"\n",
        "    if instance_prompt\n",
        "    else f\"{prompt} --n {negative}\"\n",
        ")\n",
        "\n",
        "if network_weight:\n",
        "    lora_files = [network_weight]\n",
        "\n",
        "else:\n",
        "    os.chdir(output_dir)\n",
        "    lora_files = [os.path.abspath(file) for file in glob.glob('*.safetensors')]\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "for network_weight in lora_files:\n",
        "    print(network_weight)\n",
        "    config = {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_weight\": network_weight,\n",
        "        \"network_mul\": float(network_mul),\n",
        "        \"network_args\": eval(network_args) if network_args else None,\n",
        "        \"ckpt\": inference_ckpt,\n",
        "        \"outdir\": inference_dir,\n",
        "        \"xformers\": False,\n",
        "        \"vae\": vae if vae else None,\n",
        "        \"fp16\": True,\n",
        "        \"W\": width,\n",
        "        \"H\": height,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"scale\": scale,\n",
        "        \"sampler\": sampler,\n",
        "        \"steps\": steps,\n",
        "        \"max_embeddings_multiples\": 3,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"images_per_prompt\": images_per_prompt,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"prompt\": final_prompt,\n",
        "    }\n",
        "\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        if isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        if isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        if isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    final_args = f\"python gen_img_diffusers.py {args}\"\n",
        "    !{final_args}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "muse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "365aaa7cd0f46b98e87a641efd2674b70fd0636216a71fca34e134fee9533f7d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
